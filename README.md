## Local Data Management

**⚠️ IMPORTANT: The `data/` folder is managed locally only and not tracked in git.**

To set up your local data environment:

1. **Download the Kvasir-SEG dataset** from [datasets.simula.no](https://datasets.simula.no/kvasir-seg/)
2. **Create the data structure locally:**
   ```bash
   mkdir -p data/archive
   # Extract Kvasir-SEG to data/archive/Kvasir-SEG/
   ```
3. **The data directory should look like:**
   ```
   data/
   ├─ archive/
   │  └─ Kvasir-SEG/
   │     └─ Kvasir-SEG/
   │        ├─ images/           # 1000 .jpg files
   │        └─ masks/            # 1000 .jpg mask files  
   └─ processed/                 # Generated by scripts
      ├─ images/{train,val}/     # Symlinked/copied images
      └─ labels/{train,val}/     # YOLO .txt labels
   ```

## Project Layout

```
polyp-yolo/
├─ data/                       # LOCAL ONLY - not in git
│  ├─ archive/Kvasir-SEG/      # Original downloaded dataset
│  └─ processed/               # YOLO-formatted data created by scripts
├─ models/                     # Training outputs (gitignored)
├─ results/                    # Inference outputs (gitignored) 
├─ scripts/                    # Core processing pipeline
│  ├─ convert_masks_to_yolo.py
│  ├─ split_train_val.py
│  ├─ infer_and_viz.py
│  ├─ video_infer_yolo.py
│  └─ eval_val.py
├─ yolo_data.yaml              # YOLO training configuration
├─ environment.yml
├─ requirements.txt
└─ README.md
```

## Quick Start

1. **Install dependencies:**
   ```bash
   conda env create -f environment.yml
   # OR
   pip install -r requirements.txt
   ```

2. **Set up local data:**
   ```bash
   # Download and extract Kvasir-SEG dataset to data/archive/
   mkdir -p data/archive
   # Place Kvasir-SEG dataset in data/archive/Kvasir-SEG/
   ```

3. **Convert masks to YOLO labels:**
   ```bash
   python scripts/convert_masks_to_yolo.py \
     --input_dir data/archive/Kvasir-SEG/Kvasir-SEG \
     --output_dir data/processed
   ```

4. **Split into train/val:**
   ```bash
   python scripts/split_train_val.py
   ```

5. **Train YOLO model:**
   ```bash
   yolo task=detect mode=train model=yolov8n.pt \
     data=yolo_data.yaml epochs=50 imgsz=640 batch=16 \
     name=polyp_yolov8n
   ```

6. **Run evaluation:**
   ```bash
   python scripts/eval_val.py --model models/polyp_yolov8n/weights/best.pt
   ```
   
---

Additional tools

- Video inference (run a trained model on video files):

```bash
python scripts/video_infer_yolo.py \
   --weights runs/detect/train/weights/best.pt \
   --video data/videos/sample.mp4 \
   --out results/video_out.mp4 \
   --csv results/detections.csv \
   --conf 0.25 --imgsz 640 --skip 1
```

- Multiple bbox export from masks:

The original converter writes a single bbox per image. Use the `--multi` flag to export one bbox per connected mask component instead:

```bash
python scripts/convert_masks_to_yolo.py \
   --images data/archive/Kvasir-SEG/Kvasir-SEG/images \
   --masks  data/archive/Kvasir-SEG/Kvasir-SEG/masks \
   --labels_out data/processed/labels --multi
```

- Evaluation on validation split (Ultralytics val):

```bash
python scripts/eval_val.py --weights runs/detect/train/weights/best.pt --data yolo_data.yaml --imgsz 640
```

Notes
- If you don't have real videos, you can synthesize a demo video from images using `ffmpeg`:

```bash
ffmpeg -framerate 10 -pattern_type glob -i 'data/archive/Kvasir-SEG/Kvasir-SEG/images/*.jpg' \
   -c:v libx264 -pix_fmt yuv420p results/sample_from_images.mp4
```

---
If you'd like, I can also add a small evaluation notebook and CI checks to verify scripts run in a minimal CPU environment.
